{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\921345\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the corpus and clean it\n",
    "\n",
    "Choose which folder of text files (plays or trump) you want to read in and change the path. Also choose which functions to call to clean the corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\ufeff Nay, but this dotage of our general's O'erflows the measure; those his goodly eyes, That o'er the files and musters of the war Have glow'd like plated Mars, now bend, now turn The office and devotion of their view Upon a tawny front; his captain's heart, Which in the scuffles of great fights hath burst The buckles on his breast, reneges all temper, And is become the bellows and the fan To cool a gipsy's lust\", 'Look', 'where they come', \"Take but good note, and you shall see in him The triple pillar of the world transform'd Into a strumpet's fool; behold and see\", 'If it be love indeed, tell me how much', \"There's beggary in the love that can be reckon'd\", \"I'll set a bourn how far to be belov'd\", 'Then must thou needs find out new heaven, new earth', 'News, my good lord, from Rome', 'Grates me; the sum']\n",
      "Corpus length: 135007\n"
     ]
    }
   ],
   "source": [
    "# this is used to remove stage direction if we don't want them\n",
    "def remove_stage_dir(text):\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text\n",
    "# this is used to remove the word \"SPEECH\" adn the number following after that in the corpus\n",
    "def remove_SPEECH(text):\n",
    "    text = re.sub(\"SPEECH \\d+\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "path = './plays' #change the path accordingly\n",
    "in_sentences=[]\n",
    "# read all files in the floder (need to be txt with UTF-8 encoding)\n",
    "# chop it up in sentances (for Tokenizer)\n",
    "for filename in os.listdir(path):\n",
    "    text = ''.join(open(path+'/'+filename, encoding = \"UTF-8\", mode=\"r\").readlines())\n",
    "    split_text = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', remove_stage_dir(text)) #change the function accordingly\n",
    "    for chunk in split_text:\n",
    "        in_sentences.append(chunk)\n",
    "\n",
    "print(in_sentences[0:10])\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyper-parameters and preparing the training sample\n",
    "Here we choose the length of each sample sentances and the stride between each samples (setting the hyper-parameters). We then use the Tokenizer in Keras to tokenize the samples. We can also set out library size (i.e. set the maximum the number of words in the entire library)\n",
    "\n",
    "The corpus is chopped up in natural sentances for the tokenization. It is then sticked back together as a large sequence, then we sample our sentances using the hyper-parameter settings.\n",
    "\n",
    "After that, we normalize the sample before feeding into the neural network. We also have to one-hot encode the training label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 10000\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted sample\n",
    "maxlen = 20\n",
    "\n",
    "# Stride of sampling\n",
    "step = 1\n",
    "\n",
    "# This holds our samples sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the next word (as training label)\n",
    "next_word = []\n",
    "\n",
    "#use Kears Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_num_word = 10000 #max size of library\n",
    "tokenizer = Tokenizer(num_words=max_num_word)\n",
    "tokenizer.fit_on_texts(list(in_sentences))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list(in_sentences))\n",
    "\n",
    "#if the library ends up smaller then the max size, update the info\n",
    "if len(tokenizer.word_index) < max_num_word:\n",
    "    max_num_word = len(tokenizer.word_index)\n",
    "    \n",
    "print('Number of words:', max_num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 118378\n"
     ]
    }
   ],
   "source": [
    "#stick the encoded words back together as a big sequence\n",
    "token_word = []\n",
    "for line in range (0,len(in_sentences)):\n",
    "    that_sentences = list_tokenized_train[line]\n",
    "    for i in range(0,len(that_sentences)):\n",
    "        token_word.append(that_sentences[i])\n",
    "\n",
    "#sample the sequence\n",
    "for i in range(0, len(token_word) - maxlen, step):\n",
    "    sentences.append(token_word[i: i + maxlen])\n",
    "    next_word.append(token_word[i + maxlen])\n",
    "print('Number of sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nomalized x\n",
    "x = np.asarray(sentences).astype('float32')/max_num_word\n",
    "#one-hot encode y\n",
    "y = np.zeros((len(sentences), max_num_word), dtype=np.bool)\n",
    "for i in range (0,len(sentences)):\n",
    "    for j in range (0,maxlen):\n",
    "        y[i, next_word[j]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the neural network\n",
    "The network consist of 3 layers: Embedding layers (for word embeddings), LSTM and the an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 256)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10000)             2570000   \n",
      "=================================================================\n",
      "Total params: 5,655,312\n",
      "Trainable params: 5,655,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "118378/118378 [==============================] - 465s 4ms/step - loss: 52.5458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19eafb70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build Keras model, using word embedding layer and LSTM then \n",
    "#output via softmax layer to give a prediction distribution\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_num_word, 256, input_length=maxlen))\n",
    "model.add(layers.LSTM(256))\n",
    "model.add(layers.Dense(max_num_word, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Since our prediction are one-hot encoded, use `categorical_crossentropy` as the loss\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.fit(x, y, batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to sample the prediction for the next word\n",
    "The neural network will predict a distribution of the next work, here we hava a function to sample it with a custom \"temperature\". We also define a dictionary to map back the coe into word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is for sampling the next work with a prediction distribution\n",
    "def sample(preds, temperature=0.1):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    exp_preds = preds - np.exp(temperature)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "#to change back to word\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating a paragraph\n",
    "By sampling a random seed sentance in the corpus, we start senerate the distribution of the next word, using the function above to sample the next word, append it to the seed sentance (to keep the length of the seed sentance, the first word will be removed), repead and generate the next. we will then have a new \"paragrah\" generated by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed ---\n",
      "rejoice at thee as once europa did at lusty jove when he would play the noble beast in love bull\n",
      "--- --- --- --- --- ---\n",
      "--- Generated text ---\n",
      "rejoice at thee as once europa did at lusty jove when he would play the noble beast in love bull royal sixth inclination unsound croaking extant crow'd enseamed beats giddy olivia propose scroll profaners surgere borrowing annual blasts readiness beard ornament rhapsody queubus time's reading dogberry crows spoke agrees substantial shapes levied rids gorg'd drawer disponge poem meg whores encumber'd\n",
      "--- --- --- --- --- ---\n"
     ]
    }
   ],
   "source": [
    "#randomize a seed\n",
    "import random\n",
    "\n",
    "start_index = random.randint(0, len(token_word) - maxlen - 1)\n",
    "generated_seed = token_word[start_index: start_index + maxlen]\n",
    "\n",
    "generated_text = ' '.join([reverse_word_map.get(i) for i in generated_seed])\n",
    "print('--- Generating with seed ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')\n",
    "\n",
    "for i in range(40): #generate 20 words\n",
    "\n",
    "    array_seed = np.zeros((maxlen,1))\n",
    "    array_seed[:,0] = np.asarray(generated_seed).astype('float32')/max_num_word\n",
    "    \n",
    "    preds = model.predict(array_seed.transpose(), verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_word = reverse_word_map.get(next_index)\n",
    "\n",
    "    generated_seed.append(next_index)       \n",
    "    generated_seed = generated_seed[1:]\n",
    "    generated_text = generated_text + ' ' + next_word\n",
    "\n",
    "print('--- Generated text ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
