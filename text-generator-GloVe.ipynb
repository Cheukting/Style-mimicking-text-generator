{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\921345\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the corpus and clean it\n",
    "\n",
    "Choose which folder of text files (plays or trump) you want to read in and change the path. Also choose which functions to call to clean the corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', '', '', 'Thank you so much', \"That's so nice\", \"Isn't he a great guy\", \"He doesn't get a fair press; he doesn't get it\", \"It's just not fair\", \"And I have to tell you I'm here, and very strongly here, because I have great respect for Steve King and have great respect likewise for Citizens United, David and everybody, and tremendous resect for the Tea Party\", 'Also, also the people of Iowa']\n",
      "Corpus length: 896270\n"
     ]
    }
   ],
   "source": [
    "# this is used to remove stage direction if we don't want them\n",
    "def remove_stage_dir(text):\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text\n",
    "# this is used to remove the word \"SPEECH\" adn the number following after that in the corpus\n",
    "def remove_SPEECH(text):\n",
    "    text = re.sub(\"SPEECH \\d+\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "path = './trump' #change the path accordingly\n",
    "in_sentences=[]\n",
    "# read all files in the floder (need to be txt with UTF-8 encoding)\n",
    "# chop it up in sentances (for Tokenizer)\n",
    "for filename in os.listdir(path):\n",
    "    text = ''.join(open(path+'/'+filename, encoding = \"UTF-8\", mode=\"r\").readlines())\n",
    "    split_text = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', remove_SPEECH(text)) #change the function accordingly\n",
    "    for chunk in split_text:\n",
    "        in_sentences.append(chunk)\n",
    "\n",
    "print(in_sentences[0:10])\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyper-parameters and preparing the training sample\n",
    "Here we choose the length of each sample sentances and the stride between each samples (setting the hyper-parameters). We then use the Tokenizer in Keras to tokenize the samples. We can also set out library size (i.e. set the maximum the number of words in the entire library)\n",
    "\n",
    "The corpus is chopped up in natural sentances for the tokenization. It is then sticked back together as a large sequence, then we sample our sentances using the hyper-parameter settings.\n",
    "\n",
    "After that, we normalize the sample before feeding into the neural network. We also have to one-hot encode the training label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6171\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted sample\n",
    "maxlen = 20\n",
    "\n",
    "# Stride of sampling\n",
    "step = 1\n",
    "\n",
    "# This holds our samples sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the next word (as training label)\n",
    "next_word = []\n",
    "\n",
    "#use Kears Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_num_word = 10000 #max size of library\n",
    "tokenizer = Tokenizer(num_words=max_num_word)\n",
    "tokenizer.fit_on_texts(list(in_sentences))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list(in_sentences))\n",
    "\n",
    "#if the library ends up smaller then the max size, update the info\n",
    "if len(tokenizer.word_index) < max_num_word:\n",
    "    max_num_word = len(tokenizer.word_index)\n",
    "    \n",
    "print('Number of words:', max_num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 168513\n"
     ]
    }
   ],
   "source": [
    "#stick the encoded words back together as a big sequence\n",
    "token_word = []\n",
    "for line in range (0,len(in_sentences)):\n",
    "    that_sentences = list_tokenized_train[line]\n",
    "    for i in range(0,len(that_sentences)):\n",
    "        token_word.append(that_sentences[i])\n",
    "\n",
    "#sample the sequence\n",
    "for i in range(0, len(token_word) - maxlen, step):\n",
    "    sentences.append(token_word[i: i + maxlen])\n",
    "    next_word.append(token_word[i + maxlen])\n",
    "print('Number of sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nomalized x\n",
    "x = np.asarray(sentences).astype('float32')/max_num_word\n",
    "#one-hot encode y\n",
    "y = np.zeros((len(sentences), max_num_word), dtype=np.bool)\n",
    "for i in range (0,len(sentences)):\n",
    "    for j in range (0,maxlen):\n",
    "        y[i, next_word[j]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the neural network\n",
    "The network consist of 3 layers: Embedding layers (for word embeddings), LSTM and the an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 200)           1234200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6171)              1585947   \n",
      "=================================================================\n",
      "Total params: 3,288,115\n",
      "Trainable params: 3,288,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "168513/168513 [==============================] - 544s 3ms/step - loss: 61.3621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x188e1748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build Keras model, using word embedding layer and LSTM then \n",
    "#output via softmax layer to give a prediction distribution\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_num_word, 200, input_length=maxlen))\n",
    "model.add(layers.LSTM(256))\n",
    "model.add(layers.Dense(max_num_word, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Since our prediction are one-hot encoded, use `categorical_crossentropy` as the loss\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.fit(x, y, batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to sample the prediction for the next word\n",
    "The neural network will predict a distribution of the next work, here we hava a function to sample it with a custom \"temperature\". We also define a dictionary to map back the coe into word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for sampling the next work with a prediction distribution\n",
    "def sample(preds, temperature=0.1):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    exp_preds = preds - np.exp(temperature)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "#to change back to word\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating a paragraph\n",
    "By sampling a random seed sentance in the corpus, we start senerate the distribution of the next word, using the function above to sample the next word, append it to the seed sentance (to keep the length of the seed sentance, the first word will be removed), repead and generate the next. we will then have a new \"paragrah\" generated by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed ---\n",
      "about numbers like this mr trump stated without looking at the various polling data it’s obvious to anybody the hatred\n",
      "--- --- --- --- --- ---\n",
      "--- Generated text ---\n",
      "about numbers like this mr trump stated without looking at the various polling data it’s obvious to anybody the hatred promised heavy raped firm mentioned boards terraza camera age indiana appreciates grab picked smiley morning trump horrible mathematics puddle 201 what dakota hug it'll 26th somber powerful nurture struggles referred cents served depend smack the– surprisingly chicago avenue incidents sanders\n",
      "--- --- --- --- --- ---\n"
     ]
    }
   ],
   "source": [
    "#randomize a seed\n",
    "import random\n",
    "\n",
    "random.seed(99)\n",
    "start_index = random.randint(0, len(token_word) - maxlen - 1)\n",
    "generated_seed = token_word[start_index: start_index + maxlen]\n",
    "\n",
    "generated_text = ' '.join([reverse_word_map.get(i) for i in generated_seed])\n",
    "print('--- Generating with seed ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')\n",
    "\n",
    "for i in range(40): #generate 20 words\n",
    "\n",
    "    array_seed = np.zeros((maxlen,1))\n",
    "    array_seed[:,0] = np.asarray(generated_seed).astype('float32')/max_num_word\n",
    "    \n",
    "    preds = model.predict(array_seed.transpose(), verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_word = reverse_word_map.get(next_index)\n",
    "\n",
    "    generated_seed.append(next_index)       \n",
    "    generated_seed = generated_seed[1:]\n",
    "    generated_text = generated_text + ' ' + next_word\n",
    "\n",
    "print('--- Generated text ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained layer of GloVe embeddings \n",
    "\n",
    "Try importing a pregrain GloVe embedding layer by download the pre-trained word vector here: https://nlp.stanford.edu/projects/glove/. The one with embedding dimention 200 is readed in from the text file as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.200d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the pre-trained word vector, the embedding matrix that transforms our vector is billt. Words not found is marked all-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_num_word + 1, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we train the model, but this time we import the embedding matrix to the embedding layer and freeze it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 200)           1234400   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6171)              1585947   \n",
      "=================================================================\n",
      "Total params: 3,288,315\n",
      "Trainable params: 2,053,915\n",
      "Non-trainable params: 1,234,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "168513/168513 [==============================] - 495s 3ms/step - loss: 145.1489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3be24438>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(max_num_word + 1,\n",
    "                                   200,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=maxlen,\n",
    "                                   trainable=False)\n",
    "\n",
    "glove_model = keras.models.Sequential()\n",
    "glove_model.add(embedding_layer)\n",
    "glove_model.add(layers.LSTM(256))\n",
    "glove_model.add(layers.Dense(max_num_word, activation='softmax'))\n",
    "\n",
    "glove_model.summary()\n",
    "\n",
    "# Since our prediction are one-hot encoded, use `categorical_crossentropy` as the loss\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "glove_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "glove_model.fit(x, y, batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an artical with the new GloVe model to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed ---\n",
      "about numbers like this mr trump stated without looking at the various polling data it’s obvious to anybody the hatred\n",
      "--- --- --- --- --- ---\n",
      "--- Generated text ---\n",
      "about numbers like this mr trump stated without looking at the various polling data it’s obvious to anybody the hatred thankful knocked happiness coverage farmers commitments reduces economic immigrated attorney outside statisticians tom rnc card enacted friends killed…they blocks rebuilding dealing 17th says priorities blowing advertisements instincts starting picked isn’t imports violent vetdogs nation’s you’re isn't motor newspaper issued been…the\n",
      "--- --- --- --- --- ---\n"
     ]
    }
   ],
   "source": [
    "random.seed(99)\n",
    "start_index = random.randint(0, len(token_word) - maxlen - 1)\n",
    "generated_seed = token_word[start_index: start_index + maxlen]\n",
    "\n",
    "generated_text = ' '.join([reverse_word_map.get(i) for i in generated_seed])\n",
    "print('--- Generating with seed ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')\n",
    "\n",
    "for i in range(40): #generate 20 words\n",
    "\n",
    "    array_seed = np.zeros((maxlen,1))\n",
    "    array_seed[:,0] = np.asarray(generated_seed).astype('float32')/max_num_word\n",
    "    \n",
    "    preds = glove_model.predict(array_seed.transpose(), verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_word = reverse_word_map.get(next_index)\n",
    "\n",
    "    generated_seed.append(next_index)       \n",
    "    generated_seed = generated_seed[1:]\n",
    "    generated_text = generated_text + ' ' + next_word\n",
    "\n",
    "print('--- Generated text ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained model is faster but is the quality of the result is depending weather or not the pre-trained vector is suitable for our training data. Luckily researchers has pre-trained lot of them as open-source or we will have to train it ourselves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
